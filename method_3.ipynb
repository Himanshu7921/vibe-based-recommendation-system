{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3991e15",
   "metadata": {},
   "source": [
    "## Method 3\n",
    "\n",
    "#### What happens in this method:\n",
    "\n",
    "* This method uses my **custom-built and PyPI-published library: RetrievalMind** (`pip install RetrievalMind==0.1.3`), an advanced **Retrieval-Augmented Generation (RAG)** framework.  \n",
    "* It automates the entire pipeline, including document loading, embedding generation, vector storage, retrieval, and dynamic reasoning through an integrated LLM.  \n",
    "* The system generates **AI-driven recommendations and insights dynamically**, adapting responses to the user’s query in real time.  \n",
    "* The pipeline follows a modular and production-oriented design: ensuring scalable integration between data retrieval, semantic matching, and language model reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808a5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for RAG Integration\n",
    "from RetrievalMind.embeddings_manager import EmbeddingManager\n",
    "from RetrievalMind.data_ingestion.text_ingestor import TextDocumentIngestor\n",
    "from RetrievalMind.vector_store_manager import VectorStore\n",
    "from RetrievalMind.rag_retriver import Retrieval\n",
    "import chromadb\n",
    "import uuid\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Imports for AI Agent\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.runnables import Runnable\n",
    "import os\n",
    "\n",
    "if os.getenv(\"RAG_DEBUG\", \"0\") == \"1\":\n",
    "    logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dff506",
   "metadata": {},
   "source": [
    "### Document Loading and Embedding\n",
    "\n",
    "The **`load_text_document()`** function handles loading and preparing text data for embedding and retrieval.  \n",
    "It converts raw text files into structured document chunks with metadata for downstream vectorization.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Initializes a `TextDocumentIngestor` to read and split text data.  \n",
    "2. Loads all documents from the specified folder and converts them into chunks.  \n",
    "3. Logs the number of chunks and previews sample content for verification.  \n",
    "4. Returns the processed chunks for the embedding stage.\n",
    "\n",
    "**Role in Pipeline:**  \n",
    "Serves as the **data ingestion stage**, ensuring clean, structured input for semantic embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f5485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_document(pdf_folder_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Load all PDF documents from a folder and return them as chunks.\n",
    "\n",
    "    Args:\n",
    "        pdf_folder_path (str): Path to the folder containing PDFs.\n",
    "        file_pattern (str): Glob pattern to match PDF files (default '**/*.pdf').\n",
    "\n",
    "    Returns:\n",
    "        list: List of document chunks with page content and metadata.\n",
    "    \"\"\"\n",
    "    # pdf_ingestor = PDFDocumentIngestor(file_path = pdf_folder_path, loader_type='mu')\n",
    "    text_ingestor = TextDocumentIngestor(file_path = pdf_folder_path)\n",
    "    text_loader = text_ingestor.load_document()\n",
    "    document_chunks = text_loader.load()\n",
    "    logger.debug(f\"Loaded {len(document_chunks)} document chunks from {pdf_folder_path}\")\n",
    "\n",
    "    # Debug: preview first 5 document chunks\n",
    "    for idx, chunk in enumerate(document_chunks[:5]):\n",
    "        logger.debug(f\"Document Chunk {idx} preview: {repr(chunk.page_content[:100])}\")\n",
    "    \n",
    "    return document_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b596445",
   "metadata": {},
   "source": [
    "### Document Embedding Generation\n",
    "\n",
    "The **`generate_document_embeddings()`** function encodes document chunks into high-dimensional semantic vectors using a pretrained **SentenceTransformer** model.  \n",
    "These embeddings serve as the core representations for similarity search and context retrieval.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Extracts text content from document chunks.  \n",
    "2. Initializes an `EmbeddingManager` with the chosen model (`all-MiniLM-L6-v2` by default).  \n",
    "3. Generates semantic embeddings for all text segments in batch mode.  \n",
    "4. Logs processing details, including model and chunk count.  \n",
    "5. Returns both the generated embeddings and the embedding manager.\n",
    "\n",
    "**Role in Pipeline:**  \n",
    "Transforms raw text into semantic vectors, enabling meaningful retrieval and reasoning within the RAG framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8397d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_embeddings(document_chunks: list, embedding_model: str = \"all-miniLM-L6-v2\") -> tuple:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of document chunks using a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        document_chunks (list): List of document chunks.\n",
    "        embedding_model (str): Name of the embedding model (default \"all-miniLM-L6-v2\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: embeddings list, EmbeddingManager instance\n",
    "    \"\"\"\n",
    "    texts = [chunk.page_content for chunk in document_chunks]\n",
    "    embedding_manager = EmbeddingManager(model_name=embedding_model)\n",
    "    embeddings = embedding_manager.generate_embeddings(texts)\n",
    "    logger.debug(f\"Generated embeddings for {len(texts)} chunks using model '{embedding_model}'\")\n",
    "    return embeddings, embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1d325",
   "metadata": {},
   "source": [
    "### Vector Store Creation and Persistence\n",
    "\n",
    "The **`store_documents_in_vector_store()`** function handles the storage of document embeddings and metadata in a **ChromaDB vector database**.  \n",
    "It establishes a persistent, queryable foundation for semantic retrieval in the RAG pipeline.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Creates the persistence directory if it does not exist.  \n",
    "2. Initializes a persistent ChromaDB client and collection.  \n",
    "3. Iterates through each document and embedding, generating unique IDs and metadata.  \n",
    "4. Inserts all records — documents, embeddings, and metadata — into the collection.  \n",
    "5. Logs insertion success and validates stored document count.\n",
    "\n",
    "**Role in Pipeline:**  \n",
    "Transforms generated embeddings into a **persistent vector knowledge base**, enabling scalable, reusable, and low-latency semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244c5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_documents_in_vector_store(document_chunks: list, embeddings: list, collection_name: str, persist_dir: str, doc_type: str = \"PDF\") -> object:\n",
    "    \"\"\"\n",
    "    Store document chunks and embeddings in a ChromaDB vector store.\n",
    "\n",
    "    Args:\n",
    "        document_chunks (list): List of document chunks.\n",
    "        embeddings (list): Corresponding embeddings for each chunk.\n",
    "        collection_name (str): Name of the vector store collection.\n",
    "        persist_dir (str): Directory where the vector store is persisted.\n",
    "        doc_type (str): Type of documents being stored (default \"PDF\").\n",
    "\n",
    "    Returns:\n",
    "        VectorStore: Initialized and populated vector store instance.\n",
    "    \"\"\"\n",
    "    # Ensure persistence directory exists\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize persistent ChromaDB client and collection\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"description\": f\"{doc_type} document embeddings for RAG pipeline\"}\n",
    "    )\n",
    "\n",
    "    # Prepare records for ChromaDB\n",
    "    ids, metadatas, documents_text, embeddings_list = [], [], [], []\n",
    "    for i, (doc, emb) in enumerate(zip(document_chunks, embeddings)):\n",
    "        doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "        ids.append(doc_id)\n",
    "\n",
    "        metadata = dict(getattr(doc, \"metadata\", {}))\n",
    "        metadata.update({\n",
    "            \"doc_index\": i,\n",
    "            \"content_length\": len(getattr(doc, \"page_content\", \"\")),\n",
    "            \"source\": getattr(doc, \"metadata\", {}).get(\"source\", None)\n",
    "        })\n",
    "        metadatas.append(metadata)\n",
    "\n",
    "        documents_text.append(getattr(doc, \"page_content\", \"\"))\n",
    "        embeddings_list.append(emb.tolist() if hasattr(emb, 'tolist') else list(emb))\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=documents_text,\n",
    "        metadatas=metadatas,\n",
    "        embeddings=embeddings_list\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"Successfully added {len(documents_text)} documents to '{collection_name}'.\")\n",
    "    logger.debug(f\"Total documents in collection: {collection.count()}\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9d435",
   "metadata": {},
   "source": [
    "### Querying the Vector Store\n",
    "\n",
    "The **`query_vector_store()`** function performs **semantic retrieval** from a ChromaDB vector store using a natural language query.  \n",
    "It encodes the query into an embedding, searches stored vectors for semantically similar results, and returns top-*k* matches with metadata and similarity scores.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Generate an embedding for the input query.  \n",
    "2. Query the ChromaDB collection for nearest semantic matches.  \n",
    "3. Convert distances to normalized similarity scores.  \n",
    "4. Filter low-confidence results and apply a keyword fallback if needed.  \n",
    "5. Log all retrieval steps for transparency and debugging.\n",
    "\n",
    "**Role in Pipeline:**  \n",
    "Acts as the **retrieval core** of the RAG architecture — connecting user intent to stored knowledge through semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb997d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_store(collection: object, embedding_manager: EmbeddingManager, query_text: str, top_k: int = 3, min_score: float = 0.0, raw_documents: list | None = None) -> list:\n",
    "    \"\"\"\n",
    "    Perform semantic search on the vector store.\n",
    "\n",
    "    Args:\n",
    "        vector_store (VectorStore): Initialized vector store instance.\n",
    "        embedding_manager (EmbeddingManager): Embedding generator.\n",
    "        query_text (str): Query string for retrieval.\n",
    "        top_k (int): Number of top results to return.\n",
    "        min_score (float): Minimum similarity score threshold.\n",
    "\n",
    "    Returns:\n",
    "        list: List of retrieved documents with metadata and similarity scores.\n",
    "    \"\"\"\n",
    "    # Direct Chroma query using the provided collection\n",
    "    results = []\n",
    "    try:\n",
    "        try:\n",
    "            q_embs = embedding_manager.generate_embeddings([query_text])\n",
    "            q_emb = q_embs[0] if hasattr(q_embs, '__len__') and len(q_embs) > 0 else q_embs\n",
    "            logger.debug(f\"Query embedding length: {getattr(q_emb, 'shape', None) or len(q_emb)}\")\n",
    "            logger.debug(f\"Query embedding (first 6 values): {q_emb[:6]}\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Failed to generate query embedding: {e}\")\n",
    "            raise\n",
    "\n",
    "        raw = collection.query(query_embeddings=[q_emb.tolist()], n_results=top_k)\n",
    "        raw_dist = raw.get('distances', [[]])[0] if raw.get('distances') else []\n",
    "        raw_docs = raw.get('documents', [[]])[0] if raw.get('documents') else []\n",
    "        raw_ids = raw.get('ids', [[]])[0] if raw.get('ids') else []\n",
    "        raw_metas = raw.get('metadatas', [[]])[0] if raw.get('metadatas') else []\n",
    "\n",
    "        interpreted = []\n",
    "        for idx, d in enumerate(raw_dist):\n",
    "            try:\n",
    "                similarity = 1.0 / (1.0 + float(d))\n",
    "            except Exception:\n",
    "                similarity = 0.0\n",
    "\n",
    "            if similarity >= min_score and idx < len(raw_docs):\n",
    "                interpreted.append({\n",
    "                    'id': raw_ids[idx] if idx < len(raw_ids) else f'raw-{idx}',\n",
    "                    'content': raw_docs[idx],\n",
    "                    'metadata': raw_metas[idx] if idx < len(raw_metas) else {},\n",
    "                    'similarity_score': round(similarity, 4),\n",
    "                    'distance': round(d, 4)\n",
    "                })\n",
    "\n",
    "        logger.info(f\"Direct Chroma reinterpretation returned {len(interpreted)} results\")\n",
    "        results = interpreted\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Direct Chroma query failed: {e}\")\n",
    "\n",
    "    # If still no results or results don't answer a teacher-related query, attempt a keyword fallback over raw documents\n",
    "    lower_q = query_text.lower()\n",
    "    teacher_query = any(k in lower_q for k in ['teacher', 'teach', 'instructor', 'who are the teachers'])\n",
    "\n",
    "    # If no results, or this is a teacher query and top results don't contain teacher info, run fallback\n",
    "    need_fallback = (len(results) == 0 and raw_documents) or (\n",
    "        teacher_query and raw_documents and not any(\n",
    "            any(tok in (r.get('content') or '').lower() for tok in ['teacher', 'instructor', 'himanshu', 'mihir', 'shivam'])\n",
    "            for r in results\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if need_fallback:\n",
    "        logger.warning(\"Running keyword fallback search over raw documents (teacher-related or no direct hits).\")\n",
    "        fallback = []\n",
    "        for idx, doc in enumerate(raw_documents):\n",
    "            text = getattr(doc, 'page_content', None) or (doc.get('page_content') if isinstance(doc, dict) else str(doc))\n",
    "            if not text:\n",
    "                continue\n",
    "            if any(k in text.lower() for k in ['teacher', 'instructor', 'himanshu', 'mihir', 'shivam']):\n",
    "                fallback.append({\n",
    "                    'id': f'fallback-{idx}',\n",
    "                    'similarity_score': 1.0,\n",
    "                    'content': text,\n",
    "                })\n",
    "        logger.info(f\"Fallback matched {len(fallback)} documents\")\n",
    "        if fallback:\n",
    "            return fallback\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61adab6f",
   "metadata": {},
   "source": [
    "### LLM Initialization — Google Gemini\n",
    "\n",
    "This section defines the **`initialize_llm()`** function, which configures and authenticates the **Google Gemini Large Language Model (LLM)** for use within the RAG or conversational reasoning pipeline.\n",
    "\n",
    "**Objective:**  \n",
    "To securely initialize the Gemini API client using an environment variable-stored API key, ensuring that the LLM is accessible without exposing sensitive credentials in the codebase.\n",
    "\n",
    "**Process Overview:**\n",
    "1. Retrieves the API key from a specified environment variable (`Gemini_APIKEY` by default).  \n",
    "2. Validates the existence of the key; if not found, raises a descriptive error for secure debugging.  \n",
    "3. Initializes the **`GoogleGenerativeAI`** client using the chosen Gemini model (here, `gemini-2.5-flash`), which balances speed and reasoning performance.  \n",
    "4. Returns an authenticated instance of the Gemini model, ready to handle query reasoning, summarization, or contextual answer generation.\n",
    "\n",
    "**Security and Design Notes:**\n",
    "- Storing API keys in environment variables prevents accidental exposure in version control systems.  \n",
    "- The design supports modular substitution with other models (e.g., GPT, Claude, or Llama) with minimal changes.  \n",
    "- This initialization step forms the entry point for integrating **LLM reasoning capabilities** into the retrieval-augmented generation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefe5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm(api_key_env_var: str = \"Gemini_APIKEY\") -> GoogleGenerativeAI:\n",
    "    \"\"\"\n",
    "    Initialize the Google Gemini LLM using the API key from environment variables.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(api_key_env_var)\n",
    "    if not api_key:\n",
    "        raise ValueError(f\"API key not found in environment variable '{api_key_env_var}'\")\n",
    "    return GoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c0f0b",
   "metadata": {},
   "source": [
    "### Prompt Design for the Vibe-Based Fashion Recommender\n",
    "\n",
    "This section defines the **`get_prompt()`** function, which constructs a structured **chat prompt template** to guide the behavior of the fashion recommendation LLM.  \n",
    "The prompt is designed to align the model’s responses with the tone, purpose, and style expectations of a digital fashion stylist, ensuring consistency and domain relevance across user interactions.\n",
    "\n",
    "**Objective:**  \n",
    "To define a reusable and context-aware prompt framework that enables the AI assistant to interpret user queries (vibe descriptions) and generate precise, stylistically coherent fashion recommendations based on retrieved data.\n",
    "\n",
    "**Prompt Structure:**\n",
    "1. **System Role Definition:**  \n",
    "   Establishes the assistant’s identity as a *professional AI fashion stylist* capable of interpreting fashion aesthetics and recommending suitable outfits.  \n",
    "   The model is guided to maintain an elegant, helpful, and fashion-oriented tone throughout interactions.\n",
    "\n",
    "2. **User Context Definition:**  \n",
    "   Provides the model with the retrieved product data and the user’s query (vibe description).  \n",
    "   The prompt enforces grounded reasoning — the AI must base its recommendations **only** on the provided data, avoiding fabrication or external inference.\n",
    "\n",
    "3. **Instructional Directives:**  \n",
    "   - Focus on product descriptions that match the user’s vibe.  \n",
    "   - Justify recommendations briefly (e.g., texture, color palette, or design aesthetic).  \n",
    "   - Respond with polished, concise, and brand-consistent phrasing.  \n",
    "   - Include a polite fallback response if no relevant items are found.\n",
    "\n",
    "**Design Rationale:**  \n",
    "This prompt structure ensures a balance between **creativity** and **factual grounding**, allowing the model to behave like an intelligent stylist rather than a generic chatbot.  \n",
    "It provides a controlled conversational scaffold for the RAG pipeline, ensuring that retrieved data is meaningfully transformed into high-quality recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09143ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt() -> ChatPromptTemplate:\n",
    "    \"\"\"\n",
    "    Returns a ChatPromptTemplate for the RAG AI assistant specialized in company policies.\n",
    "    \"\"\"\n",
    "    return ChatPromptTemplate([\n",
    "        ('system', \"\"\"You are a helpful and knowledgeable AI fashion stylist and product recommender for an online fashion platform. \n",
    "        Your role is to assist users in discovering fashion items that match their personal vibe, mood, or style preferences. \n",
    "        You understand aesthetic themes (like 'urban chic', 'boho casual', or 'minimal elegance') and can recommend matching outfits from the retrieved product data. \n",
    "        Always maintain a professional, engaging, and style-oriented tone — similar to a fashion consultant.\"\"\"),\n",
    "\n",
    "\n",
    "        ('user', \"\"\"Here are the retrieved product descriptions and metadata relevant to the user's vibe or style preference:\n",
    "        {retrieved_docs_from_rag}\n",
    "        User Query: {user_query}\n",
    "\n",
    "        Instructions:\n",
    "\n",
    "        - Recommend the most relevant fashion items based **only** on the retrieved product information.\n",
    "        - Do **not** fabricate or assume product details not present in the data above.\n",
    "        - If no relevant match is found, respond politely by saying: \n",
    "        \"It seems we don’t have a perfect match for this vibe right now. You can try a different style keyword, or check back soon for new arrivals!\"\n",
    "        - Keep the tone sophisticated yet accessible, similar to a modern online stylist or fashion consultant.\n",
    "        - Mention why an item fits the described vibe (e.g., color palette, texture, aesthetic, or design).\n",
    "        - Keep responses concise, elegant, and engaging.\n",
    "        - Please provide the answer in plain text format without using markdown, bold text, or any special formatting.\n",
    "        Provide your recommendation below:\"\"\")\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde7159",
   "metadata": {},
   "source": [
    "### Chain Construction\n",
    "\n",
    "This function, **`get_chain()`**, creates an executable RAG pipeline by linking the prompt template, language model, and output parser into a single runnable chain.  \n",
    "It ensures seamless flow — the prompt feeds into the LLM, and the generated response is parsed into a clean, human-readable output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf437bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(llm: GoogleGenerativeAI, prompt_template: ChatPromptTemplate) -> Runnable:\n",
    "    \"\"\"\n",
    "    Create a Runnable chain combining the prompt template, LLM, and output parser.\n",
    "    \"\"\"\n",
    "    return prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524ae7b",
   "metadata": {},
   "source": [
    "### Query Execution with RAG\n",
    "\n",
    "The **`ask_with_rag()`** function generates the final AI response by combining the user’s query with retrieved context documents.  \n",
    "It feeds both into the runnable chain, enabling the LLM to produce a context-aware and grounded recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e300d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_rag(chain: Runnable, query: str, retrieved_docs: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate an AI answer for a query based on RAG retrieved documents.\n",
    "    \"\"\"\n",
    "    docs_text = \"\\n\".join([doc['content'] for doc in retrieved_docs])\n",
    "    input_mapping = {\n",
    "        \"user_query\": query,\n",
    "        \"retrieved_docs_from_rag\": docs_text\n",
    "    }\n",
    "    return chain.invoke(input_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee5136",
   "metadata": {},
   "source": [
    "### Main RAG Pipeline\n",
    "\n",
    "The **`main()`** function executes the complete RAG workflow — loading text data, generating embeddings, storing them in a vector database, and retrieving the most relevant documents for a given query.  \n",
    "It integrates all key components (embedding, storage, and retrieval) into a unified, end-to-end pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087c3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query: str):\n",
    "    \"\"\"\n",
    "    Full RAG pipeline: load documents, generate embeddings, store/retrieve, and prepare for LLM query.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    text_folder_path = \"data.txt\"\n",
    "    vector_collection_name = \"nexora_collection\"\n",
    "    vector_store_directory = \"data/nexora_vector_store\"\n",
    "\n",
    "    # Load PDF documents (example: \"Travel Policy\", \"Expense Policy\", \"HR Guidelines\")\n",
    "    text_chunks = load_text_document(text_folder_path)\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings, embedding_manager = generate_document_embeddings(text_chunks)\n",
    "\n",
    "    # Store documents in vector store using RetrievalMind's VectorStore\n",
    "    try:\n",
    "        vector_store = VectorStore(collection_name=vector_collection_name, persist_directory=vector_store_directory, document_type=\"PDF\")\n",
    "        vector_store.add_document(documents=text_chunks, embeddings=embeddings)\n",
    "        logger.debug(f\"Added {len(text_chunks)} documents to RetrievalMind VectorStore '{vector_collection_name}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to store documents in RetrievalMind VectorStore: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Retrieve relevant documents using RetrievalMind's Retrieval\n",
    "    try:\n",
    "        retrieval_pipeline = Retrieval(vector_store=vector_store, embedding_manager=embedding_manager)\n",
    "        retrieved_docs = retrieval_pipeline.retrieve(query=query, top_k=5, score_threshold=0)\n",
    "        logger.info(f\"RetrievalMind returned {len(retrieved_docs)} results\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"RetrievalMind retrieval failed: {e}\")\n",
    "        retrieved_docs = []\n",
    "\n",
    "    # Preview retrieved docs\n",
    "    for doc in retrieved_docs:\n",
    "        logger.debug(\"Retrieved Document ID: %s\", doc.get('id'))\n",
    "        logger.debug(\"Similarity Score: %s\", doc.get('similarity_score'))\n",
    "        logger.debug(\"Content Preview: %s...\", (doc.get('content') or '')[:500])\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc772e8",
   "metadata": {},
   "source": [
    "### End-to-End Execution\n",
    "\n",
    "This block runs the complete RAG pipeline: retrieves context documents, initializes the LLM, builds the prompt chain, and generates the final vibe-based fashion recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc09358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vector store ready: 'nexora_collection'\n",
      "[INFO] Current document count: 1\n",
      "[INFO] No new documents to add to 'nexora_collection'. Skipped 1 duplicates.\n",
      "--------------------------------------------------------------------------------\n",
      "Query:  energetic urban chic\n",
      "Recommendation:  For an energetic urban chic vibe, I have curated a selection of items that perfectly embody this dynamic aesthetic.\n",
      "\n",
      "First, the Street Hoodie is an excellent choice. Its graffiti prints and bold typography instantly resonate with an energetic urban feel, offering a youthful edge that truly energizes your look.\n",
      "\n",
      "Next, consider the Athletic Joggers. These slim-fit joggers are ideal for urban streetwear looks, blending performance with everyday versatility, which perfectly aligns with the active and energetic dimension of your desired style.\n",
      "\n",
      "Finally, to infuse that essential 'chic' element with an energetic twist, the Leather Biker Jacket is an impeccable piece. Its edgy design, complete with silver zippers and quilted shoulders, exudes a rebellious charm and chic boldness, elevating your ensemble with sophisticated urban flair.\n"
     ]
    }
   ],
   "source": [
    "query = \"energetic urban chic\"\n",
    "\n",
    "retrieved_docs = main(query=query)\n",
    "logger.debug(\"retrieved_docs: %s\", retrieved_docs)\n",
    "\n",
    "llm_model = initialize_llm()\n",
    "\n",
    "prompt_template = get_prompt()\n",
    "rag_chain = get_chain(llm_model, prompt_template)\n",
    "\n",
    "# Step 7: Ask query\n",
    "answer = ask_with_rag(rag_chain, query=query, retrieved_docs=retrieved_docs)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Query: \", query)\n",
    "print(\"Recommendation: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
